#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Build a multi-level category hierarchy for a dataset (generated by an LLM) and save as JSON.
"""

import os
import json
import time
import argparse
from typing import List, Dict, Any, Tuple, Set, Optional
import requests

def get_subfolder_names(directory_path: str) -> List[str]:
    if not os.path.isdir(directory_path):
        raise FileNotFoundError(f"dataset_dir does not exist: {directory_path}")
    names = []
    for item in os.listdir(directory_path):
        p = os.path.join(directory_path, item)
        if os.path.isdir(p):
            names.append(item)
    names.sort()
    return names

DEFAULT_INSTRUCTION = (
    "1. Please determine your expert identity based on the categories in this dataset. "
    "For example, if the dataset is about birds, then you should assume the role of a bird expert.\n"
    "2. Please automatically construct a reasonable multi-level semantic taxonomy tree "
    "based on the given list of category names. During the organization process, "
    "prioritize grouping categories that share similar visual features (e.g., shape, color, texture, contour) "
    "and functional attributes (e.g., purpose, mode of operation, structural design, operating environment) "
    "under the most specific and semantically appropriate common parent node. "
    "Adhere to the 'minimal superordinate principle' to avoid excessive abstraction or overly fine-grained divisions, "
    "while ensuring semantic coherence and logical consistency across all levels of the hierarchy.\n"
    "3. The output MUST be STRICT JSON following the schema below. "
    "Use unique integer 'id's (0..N). A leaf node must have type='leaf' and no 'children'. "
    "An internal node must have type='internal' and a non-empty 'children' array.\n"
)

DEFAULT_SCHEMA_HINT = (
    "JSON schema (example):\n"
    "{\n"
    "  \"id\": 0,\n"
    "  \"label\": \"root\",\n"
    "  \"type\": \"internal\",\n"
    "  \"children\": [\n"
    "    {\n"
    "      \"id\": 1,\n"
    "      \"label\": \"Group A\",\n"
    "      \"type\": \"internal\",\n"
    "      \"children\": [\n"
    "        { \"id\": 2, \"label\": \"Class 1\", \"type\": \"leaf\" },\n"
    "        { \"id\": 3, \"label\": \"Class 2\", \"type\": \"leaf\" }\n"
    "      ]\n"
    "    },\n"
    "    {\n"
    "      \"id\": 4,\n"
    "      \"label\": \"Group B\",\n"
    "      \"type\": \"internal\",\n"
    "      \"children\": [\n"
    "        { \"id\": 5, \"label\": \"Class 3\", \"type\": \"leaf\" },\n"
    "        { \"id\": 6, \"label\": \"Class 4\", \"type\": \"leaf\" }\n"
    "      ]\n"
    "    }\n"
    "  ]\n"
    "}\n"
)

def build_prompt(categories: List[str], user_prompt_template: str = None) -> str:
    cats_block = "\n".join(f"- {c}" for c in categories)
    if user_prompt_template is None:
        user_prompt_template = (
            "{instruction}\n\n"
            "{schema}\n"
            "Here is the category list (one per line):\n"
            "{cats}\n\n"
            "Return the hierarchy as STRICT JSON now."
        )
    return user_prompt_template.format(
        instruction=DEFAULT_INSTRUCTION,
        schema=DEFAULT_SCHEMA_HINT,
        cats=cats_block
    )

def call_llm(
    provider: str,
    model: str,
    prompt: str,
    temperature: float = 0.5,
    max_retries: int = 3,
    timeout_s: int = 120,
    api_key: Optional[str] = None,
    api_url: Optional[str] = None,
    max_tokens: Optional[int] = None
) -> str:
    provider = provider.lower().strip()
    if provider not in {"openai", "openrouter"}:
        raise ValueError("provider must be 'openai' or 'openrouter'")

    # Choose default URL and default key source
    if provider == "openai":
        default_url = "https://api.openai.com/v1/chat/completions"
        if not api_key:
            api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("API key missing. Provide via --api_key or set OPENAI_API_KEY")
    else:
        default_url = "https://openrouter.ai/api/v1/chat/completions"
        if not api_key:
            api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise RuntimeError("API key missing. Provide via --api_key or set OPENROUTER_API_KEY")

    url = api_url or default_url
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    payload: Dict[str, Any] = {
        "model": model,
        "temperature": temperature,
        "messages": [
            {"role": "system", "content": "You only respond with valid JSON."},
            {"role": "user", "content": prompt}
        ]
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens

    # Request + retry
    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout_s)
            resp.raise_for_status()
            data = resp.json()

            # Parse content; support content as string or segmented list
            choices = data.get("choices", [])
            if not choices:
                raise RuntimeError(f"LLM returned unexpected structure: {data}")

            message = choices[0].get("message", {})
            content = message.get("content", "")

            if isinstance(content, list):
                # Concatenate segments if the content is a list
                content = "".join(
                    (seg.get("text", "") if isinstance(seg, dict) else str(seg)) for seg in content
                )

            if not isinstance(content, str):
                content = str(content)

            return content.strip()

        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(1.5 * attempt)
            else:
                raise RuntimeError(f"LLM request failed: {e}") from e

def _strip_code_fences(s: str) -> str:
    """Remove ```json ... ``` or ``` ... ``` fences."""
    text = s.strip()
    if text.startswith("```"):
        text = text.strip("`")
        if text.startswith("json"):
            text = text[4:]
    return text.strip()

def parse_llm_json(raw: str) -> Dict[str, Any]:
    """Try direct json.loads; if it fails, strip code fences and try again."""
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        cleaned = _strip_code_fences(raw)
        return json.loads(cleaned)

def validate_and_report(hierarchy: Dict[str, Any], categories: List[str]) -> Tuple[Set[str], Set[str]]:
    """
    Adapted to the new schema:
    - The top-level is the root node (with id/label/type/children)
    - Use type == "leaf" to identify leaves; check their label against the category set
    Returns: (used_labels, unknown_labels)
    """
    cat_set = set(categories)
    used: Set[str] = set()
    unknown: Set[str] = set()

    # Be tolerant to {"root": {...}} wrapping if it appears
    root = hierarchy.get("root", hierarchy)
    if not isinstance(root, dict):
        raise ValueError("The hierarchy root must be an object (either direct root or {'root': {...}}).")

    def dfs(node: Dict[str, Any]):
        node_type = node.get("type")
        if node_type == "leaf":
            label = node.get("label")
            if not isinstance(label, str):
                raise ValueError("Leaf node 'label' must be a string")
            if label in cat_set:
                used.add(label)
            else:
                unknown.add(label)
            return

        # Internal node
        if node_type == "internal":
            children = node.get("children", [])
            if not isinstance(children, list):
                raise ValueError("'children' must be a list")
            # Soft warning: internal node should have children
            for ch in children:
                if not isinstance(ch, dict):
                    raise ValueError("Each child must be an object")
                dfs(ch)
        else:
            # Tolerant fallback: infer by presence of children
            if "children" in node:
                for ch in node.get("children", []):
                    dfs(ch)
            else:
                label = node.get("label")
                if isinstance(label, str):
                    (used if label in cat_set else unknown).add(label)
                else:
                    raise ValueError("Node lacks valid type/children/label information")

    dfs(root)
    return used, unknown

def save_hierarchy_json(
    output_dir: str,
    dataset_name: str,
    hierarchy: Dict[str, Any],
    meta: Dict[str, Any] = None
) -> str:
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, f"{dataset_name}.json")
    out_data = {
        "dataset": dataset_name,
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "hierarchy": hierarchy
    }
    if meta:
        out_data["meta"] = meta
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(out_data, f, ensure_ascii=False, indent=2)
    return out_path

def main():
    parser = argparse.ArgumentParser(description="Build a dataset category hierarchy with an LLM and save as JSON")
    parser.add_argument("--dataset_dir", required=True, help="Dataset root (contains subfolders as classes)")
    parser.add_argument("--output_dir", required=True, help="Directory to save the output JSON")
    parser.add_argument("--provider", default="openrouter", choices=["openai", "openrouter"], help="LLM provider")
    parser.add_argument("--model", default="openai/gpt-4o-2024-11-20", help="LLM model name")
    parser.add_argument("--prompt_file", default=None, help="Custom prompt template file (optional)")
    parser.add_argument("--temperature", type=float, default=0.5, help="LLM sampling temperature")
    parser.add_argument("--api_key", default=None, help="API key (optional; falls back to env var)")
    parser.add_argument("--api_url", default=None, help="Custom API base URL (optional; defaults by provider)")
    parser.add_argument("--max_tokens", type=int, default=None, help="Optional max tokens limit")
    args = parser.parse_args()

    dataset_dir = os.path.abspath(args.dataset_dir)
    output_dir = os.path.abspath(args.output_dir)
    dataset_name = os.path.basename(os.path.dirname(os.path.normpath(dataset_dir)))

    categories = get_subfolder_names(dataset_dir)
    if not categories:
        raise RuntimeError(f"No subfolders (classes) found under {dataset_dir}")
    print(f"[INFO] Found {len(categories)} classes. Example: {categories[:10]}")

    user_prompt_template = None
    if args.prompt_file:
        with open(args.prompt_file, "r", encoding="utf-8") as f:
            user_prompt_template = f.read()
    prompt = build_prompt(categories, user_prompt_template)

    raw = call_llm(
        provider=args.provider,
        model=args.model,
        prompt=prompt,
        temperature=args.temperature,
        api_key=args.api_key,
        api_url=args.api_url,
        max_tokens=args.max_tokens
    )

    hierarchy = parse_llm_json(raw)
    used, unknown = validate_and_report(hierarchy, categories)
    missing = set(categories) - used
    if unknown:
        print(f"[WARN] Labels not in dataset (possible hallucinations): {sorted(list(unknown))[:20]}")
    if missing:
        print(f"[WARN] {len(missing)} dataset classes not covered. Examples: {sorted(list(missing))[:20]}")

    meta = {
        "provider": args.provider,
        "model": args.model,
        "temperature": args.temperature,
        "total_classes": len(categories),
        "used_classes": len(used),
        "unknown_classes": sorted(list(unknown)),
        "missing_classes": sorted(list(missing))
    }
    out_path = save_hierarchy_json(output_dir, dataset_name, hierarchy, meta)
    print(f"[OK] Saved: {out_path}")


if __name__ == "__main__":
    main()
